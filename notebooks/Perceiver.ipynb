{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5359a05e",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "    body {\n",
    "        font-family: Arial, sans-serif; /* Change the font to Arial */\n",
    "    }\n",
    "    \n",
    "    .red-text {\n",
    "        color: red; /* Change the text color to red */\n",
    "    }\n",
    "    \n",
    "    .blue-text {\n",
    "        color: blue; /* Change the text color to blue */\n",
    "    }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "\n",
    "<div>\n",
    "    <div style=\"width: 200; float: left;\">\n",
    "        <h1 style=\"color: #591101;  font-family: Arial, sans-serif; font-weight: 900;\">Perceiver Explained</h1>\n",
    "        <p style=\"color: #878787;  font-family: Arial, sans-serif; font-weight: 700;\">Perceiver: General Perception with Iterative Attention</p>\n",
    "        <hr>\n",
    "        <h4>Author:</h4>\n",
    "        <pre>➢ Clint Morris</pre>\n",
    "        <h4>Publication:</h4>\n",
    "        <pre><a href=\"https://arxiv.org/pdf/2103.03206.pdf\">➢ arXiv:2103.03206</a></pre>\n",
    "        <h4>Code Base:</h4>\n",
    "        <pre><a href=\"https://github.com/lucidrains/perceiver-pytorch/blob/main/perceiver_pytorch/perceiver_pytorch.py\">➢ Pytorch - Phil Wang</a>\n",
    "<a href=\"https://github.com/lucidrains/perceiver-pytorch/blob/main/perceiver_pytorch/perceiver_pytorch.py\">➢ Tensorflow - Clint Morris</a></pre>\n",
    "            <br><br>\n",
    "            <img src=\"https://i.ibb.co/k9dtKFm/Capture.png\"  width=\"450\">\n",
    "            <img src=\"https://i.ibb.co/gTtgNfY/My-project.png\" width=\"120\" style=\"position: absolute; bottom: 0; left: 0;\">\n",
    "    </div>\n",
    "    <div style=\"width: 500; float: right;\">\n",
    "        <img src=\"https://s10.gifyu.com/images/demo38de86fabd82634d.gif\"  width=\"500\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7097df01",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<h2>Stage 1 - Cross Attention</h2>\n",
    "<hr>\n",
    "<img src=\"https://i.ibb.co/Y7mtjh9/perceiver-drawio2-Copy.png\"  width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527d4e5",
   "metadata": {},
   "source": [
    "<h3>Stage 1.1 - PreNorm</h3>\n",
    "\n",
    "The PreNorm class is a custom PyTorch module that applies layer normalization to the input (and optionally to the context) before passing it to the specified function or layer. This can be useful in improving the training process of deep learning models by normalizing the input across features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35171949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, context_dim = None):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(self.norm_context):\n",
    "            context = kwargs['context']\n",
    "            normed_context = self.norm_context(context)\n",
    "            kwargs.update(context = normed_context)\n",
    "\n",
    "        return self.fn(x, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c1e0a3",
   "metadata": {},
   "source": [
    "<h3>Stage 1.2 - Attention</h3>\n",
    "\n",
    "The PreNorm class is a custom PyTorch module that applies layer normalization to the input (and optionally to the context) before passing it to the specified function or layer. This can be useful in improving the training process of deep learning models by normalizing the input across features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00411d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim = None, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
    "\n",
    "    def forward(self, x, context = None, mask = None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b ... -> b (...)')\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, 'b j -> (b h) () j', h = h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        # attention, what we cannot get enough of\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a7424",
   "metadata": {},
   "source": [
    "<h4>Stage 1.2.1 - chunk demo</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc4771f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "Chunk 1:\n",
      "tensor([[1, 2],\n",
      "        [5, 6]])\n",
      "Chunk 2:\n",
      "tensor([[3, 4],\n",
      "        [7, 8]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 2x4 tensor\n",
    "tensor = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8]\n",
    "])\n",
    "\n",
    "# Split the tensor into 2 chunks along the last dimension (columns)\n",
    "chunks = torch.chunk(tensor, chunks=2, dim=-1)\n",
    "\n",
    "# Print the original tensor and the resulting chunks\n",
    "print(\"Original Tensor:\")\n",
    "print(tensor)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i + 1}:\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52661341",
   "metadata": {},
   "source": [
    "<h4>Stage 1.2.2 - rearrange demo</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1d84324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor (shape: torch.Size([2, 2, 4])):\n",
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8]],\n",
      "\n",
      "        [[ 9, 10, 11, 12],\n",
      "         [13, 14, 15, 16]]])\n",
      "\n",
      "Rearranged Output Tensor (shape: torch.Size([4, 2, 2])):\n",
      "tensor([[[ 1,  2],\n",
      "         [ 5,  6]],\n",
      "\n",
      "        [[ 3,  4],\n",
      "         [ 7,  8]],\n",
      "\n",
      "        [[ 9, 10],\n",
      "         [13, 14]],\n",
      "\n",
      "        [[11, 12],\n",
      "         [15, 16]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "# Input tensor shape: (batch_size, num_items, num_groups * group_dim)\n",
    "tensor = torch.tensor([\n",
    "    [\n",
    "        [1, 2, 3, 4],\n",
    "        [5, 6, 7, 8]\n",
    "    ],\n",
    "    [\n",
    "        [9, 10, 11, 12],\n",
    "        [13, 14, 15, 16]\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Parameters:\n",
    "batch_size = 2\n",
    "num_items = 2\n",
    "num_groups = 2\n",
    "group_dim = 2\n",
    "\n",
    "# Rearrange the tensor using the specified pattern\n",
    "output = rearrange(tensor, 'b n (h d) -> (b h) n d', h=num_groups)\n",
    "\n",
    "# Print the input tensor and the rearranged output tensor\n",
    "print(\"Input Tensor (shape: {}):\".format(tensor.shape))\n",
    "print(tensor)\n",
    "print(\"\\nRearranged Output Tensor (shape: {}):\".format(output.size()))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f4e7195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 2, 4]):\n",
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8]],\n",
      "\n",
      "        [[ 9, 10, 11, 12],\n",
      "         [13, 14, 15, 16]]])\n",
      "\n",
      "Rearranged Output shape: torch.Size([4, 2, 2]):\n",
      "tensor([[[ 1,  2],\n",
      "         [ 5,  6]],\n",
      "\n",
      "        [[ 3,  4],\n",
      "         [ 7,  8]],\n",
      "\n",
      "        [[ 9, 10],\n",
      "         [13, 14]],\n",
      "\n",
      "        [[11, 12],\n",
      "         [15, 16]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Input tensor shape: (batch_size, num_items, num_groups * group_dim)\n",
    "tensor = torch.tensor([\n",
    "                        [[1, 2, 3, 4],\n",
    "                         [5, 6, 7, 8]],\n",
    "                        [[9, 10, 11, 12],\n",
    "                         [13, 14, 15, 16]]\n",
    "                      ])\n",
    "\n",
    "# Parameters:\n",
    "batch_size = 2\n",
    "num_items = 2\n",
    "num_groups = 2\n",
    "group_dim = 2\n",
    "\n",
    "# Rearrange the tensor using PyTorch view() and permute()\n",
    "reshaped_tensor = tensor.view(batch_size, num_items, num_groups, group_dim)\n",
    "output = reshaped_tensor.permute(0, 2, 1, 3).contiguous().view(batch_size * num_groups, num_items, group_dim)\n",
    "\n",
    "# Print the input tensor and the rearranged output tensor\n",
    "print(\"Input shape: {}:\".format(tensor.shape))\n",
    "print(tensor)\n",
    "print(\"\\nRearranged Output shape: {}:\".format(output.shape))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6702b709",
   "metadata": {},
   "source": [
    "<h4>Stage 1.2.3 - einsum demo</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5c83475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn shape: torch.Size([2, 2, 2])\n",
      "value shape: torch.Size([2, 2, 3])\n",
      "\n",
      "Result shape: torch.Size([2, 2, 3]):\n",
      "tensor([[[2.0000, 3.0000, 4.0000],\n",
      "         [2.4000, 3.4000, 4.4000]],\n",
      "\n",
      "        [[6.6000, 7.6000, 8.8000],\n",
      "         [6.2000, 7.2000, 8.6000]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Input tensors\n",
    "attn = torch.tensor([\n",
    "    [\n",
    "        [0.5, 0.5],\n",
    "        [0.3, 0.7]\n",
    "    ],\n",
    "    [\n",
    "        [0.2, 0.8],\n",
    "        [0.4, 0.6]\n",
    "    ]\n",
    "])\n",
    "\n",
    "print(f'attn shape: {attn.shape}')\n",
    "\n",
    "v = torch.tensor([\n",
    "    [\n",
    "        [1., 2., 3.],\n",
    "        [3., 4., 5.]\n",
    "    ],\n",
    "    [\n",
    "        [5., 6., 8.],\n",
    "        [7., 8., 9.]\n",
    "    ]\n",
    "])\n",
    "\n",
    "print(f'value shape: {v.shape}\\n')\n",
    "\n",
    "# Perform the einsum operation\n",
    "result = torch.einsum('b i j, b j d -> b i d', attn, v)\n",
    "\n",
    "print(\"Result shape: {}:\".format(result.shape))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c84cd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn shape: torch.Size([2, 4, 2])\n",
      "value shape: torch.Size([2, 2, 1])\n",
      "\n",
      "Result Tensor (shape: torch.Size([2, 4, 1])):\n",
      "tensor([[[4.0000],\n",
      "         [4.4000],\n",
      "         [4.2000],\n",
      "         [4.2000]],\n",
      "\n",
      "        [[8.8000],\n",
      "         [8.6000],\n",
      "         [8.6000],\n",
      "         [8.6000]]])\n"
     ]
    }
   ],
   "source": [
    "attn = torch.tensor([\n",
    "    [\n",
    "        [0.5, 0.5],\n",
    "        [0.3, 0.7],\n",
    "        [0.4, 0.6],\n",
    "        [0.4, 0.6]\n",
    "    ],\n",
    "    [\n",
    "        [0.2, 0.8],\n",
    "        [0.4, 0.6],\n",
    "        [0.4, 0.6],\n",
    "        [0.4, 0.6]\n",
    "    ]\n",
    "])\n",
    "\n",
    "print(f'attn shape: {attn.shape}')\n",
    "\n",
    "v = torch.tensor([\n",
    "    [\n",
    "        [3.],\n",
    "        [5.]\n",
    "    ],\n",
    "    [\n",
    "        [8.],\n",
    "        [9.]\n",
    "    ]\n",
    "])\n",
    "\n",
    "print(f'value shape: {v.shape}\\n')\n",
    "\n",
    "# Perform the einsum operation\n",
    "result = torch.einsum('b i j, b j d -> b i d', attn, v)\n",
    "\n",
    "print(\"Result Tensor (shape: {}):\".format(result.shape))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e60dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cross_attn = lambda: PreNorm(latent_dim, \n",
    "                                 Attention(latent_dim, input_dim, heads = cross_heads, dim_head = cross_dim_head, \n",
    "                                           dropout = attn_dropout), \n",
    "                                 context_dim = input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d7f8f5",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<h2>Stage 3 - Cross Attention FFN</h2>\n",
    "<hr>\n",
    "<img src=\"https://i.ibb.co/th9399Z/perceiver-drawio2-1.png\"  width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e8a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim = -1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Linear(dim * mult, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1261d9",
   "metadata": {},
   "source": [
    "The Gaussian Error Linear Unit (GELU) is an activation function used in neural networks. It was proposed by Hendrycks and Gimpel in their 2016 paper titled \"Gaussian Error Linear Units (GELUs)\" (https://arxiv.org/abs/1606.08415). The GELU function has gained popularity in recent years, especially in transformer-based architectures such as BERT, due to its various advantages.\n",
    "\n",
    "Some benefits of using GELU are:\n",
    "\n",
    "- Smoothness: GELU is a smooth, differentiable function, which makes it suitable for gradient-based optimization algorithms used in deep learning.\n",
    "- Non-monotonic: GELU is non-monotonic around the origin, meaning that it can model both positive and negative relationships between variables.\n",
    "- Better gradient propagation: Compared to other activation functions like ReLU, GELU tends to have better gradient propagation through the layers of the network. This is because the GELU function has non-zero gradients for both positive and negative input values, reducing the likelihood of the \"dying ReLU\" problem, where a neuron's gradient becomes zero and the neuron stops contributing to the learning process.\n",
    "- Improved performance: In many cases, GELU has been shown to improve the performance of neural networks, especially in transformer-based architectures, compared to other activation functions like ReLU or leaky ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05357fd",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/v2/resize:fit:623/0*lpKy2FLJ8NV0QkGY\"  width=\"500\">\n",
    "<img src=\"https://cvml-expertguide.net/wp-content/uploads/2021/08/activation_functions-1.png\"  width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cf1a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cross_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim, dropout = ff_dropout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefe6ccd",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<h2>Stage 4 - Latent Attention</h2>\n",
    "<hr>\n",
    "<img src=\"https://i.ibb.co/Kqxkq06/perceiver-drawio2-Copy2.png\"  width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f073bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_latent_attn = lambda: PreNorm(latent_dim, \n",
    "                                  Attention(latent_dim, heads = latent_heads, dim_head = latent_dim_head, \n",
    "                                            dropout = attn_dropout)\n",
    "                                 )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb406c68",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<h2>Stage 5 - Latent Attention FFN</h2>\n",
    "<hr>\n",
    "<img src=\"https://i.ibb.co/RYXXqvm/perceiver-drawio2-Copy3.png\"  width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim, dropout = ff_dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeff264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fcf321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052aa548",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rearrange(data, 'b ... d -> b (...) d')\n",
    "x = repeat(self.latents, 'n d -> b n d', b = b)\n",
    "# layers\n",
    "for cross_attn, cross_ff, self_attns in self.layers:\n",
    "    x = cross_attn(x, context = data, mask = mask) + x\n",
    "    x = cross_ff(x) + x\n",
    "\n",
    "    for self_attn, self_ff in self_attns:\n",
    "        x = self_attn(x) + x\n",
    "        x = self_ff(x) + x\n",
    "        \n",
    "if return_embeddings:\n",
    "    return x\n",
    "\n",
    "# to logits\n",
    "\n",
    "return self.to_logits(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28070691",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.to_logits = nn.Sequential(\n",
    "    Reduce('b n d -> b d', 'mean'),\n",
    "    nn.LayerNorm(latent_dim),\n",
    "    nn.Linear(latent_dim, num_classes)\n",
    ") if final_classifier_head else nn.Identity()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
